{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7514053 [[-0.30839643]\n",
      " [-0.7010044 ]]\n",
      "100 0.72408354 [[-0.23629944]\n",
      " [-0.6064364 ]]\n",
      "200 0.7105506 [[-0.18622331]\n",
      " [-0.53450173]]\n",
      "300 0.7039235 [[-0.15196581]\n",
      " [-0.47938278]]\n",
      "400 0.7006171 [[-0.12869057]\n",
      " [-0.4363741 ]]\n",
      "500 0.6988808 [[-0.11289472]\n",
      " [-0.4019856 ]]\n",
      "600 0.6978876 [[-0.10213277]\n",
      " [-0.37373605]]\n",
      "700 0.6972523 [[-0.09472948]\n",
      " [-0.3498958 ]]\n",
      "800 0.6967963 [[-0.08954912]\n",
      " [-0.32927015]]\n",
      "900 0.6964369 [[-0.08582686]\n",
      " [-0.31103608]]\n",
      "1000 0.69613475 [[-0.08305021]\n",
      " [-0.294625  ]]\n",
      "1100 0.69587094 [[-0.08087764]\n",
      " [-0.27964234]]\n",
      "1200 0.6956358 [[-0.07908266]\n",
      " [-0.26581213]]\n",
      "1300 0.69542366 [[-0.07751641]\n",
      " [-0.25293854]]\n",
      "1400 0.6952313 [[-0.07608186]\n",
      " [-0.24088061]]\n",
      "1500 0.69505614 [[-0.0747168 ]\n",
      " [-0.22953475]]\n",
      "1600 0.6948965 [[-0.07338215]\n",
      " [-0.21882305]]\n",
      "1700 0.69475085 [[-0.0720541 ]\n",
      " [-0.20868553]]\n",
      "1800 0.69461775 [[-0.07071918]\n",
      " [-0.19907424]]\n",
      "1900 0.69449615 [[-0.06937032]\n",
      " [-0.18994993]]\n",
      "2000 0.69438493 [[-0.06800478]\n",
      " [-0.18127944]]\n",
      "2100 0.69428325 [[-0.06662247]\n",
      " [-0.17303461]]\n",
      "2200 0.6941902 [[-0.06522498]\n",
      " [-0.1651901 ]]\n",
      "2300 0.694105 [[-0.06381486]\n",
      " [-0.15772332]]\n",
      "2400 0.694027 [[-0.06239517]\n",
      " [-0.15061381]]\n",
      "2500 0.69395554 [[-0.06096916]\n",
      " [-0.14384256]]\n",
      "2600 0.6938901 [[-0.05954014]\n",
      " [-0.13739207]]\n",
      "2700 0.6938302 [[-0.05811127]\n",
      " [-0.13124597]]\n",
      "2800 0.69377506 [[-0.0566856 ]\n",
      " [-0.12538873]]\n",
      "2900 0.6937247 [[-0.05526595]\n",
      " [-0.11980596]]\n",
      "3000 0.6936784 [[-0.05385493]\n",
      " [-0.11448404]]\n",
      "3100 0.69363594 [[-0.05245491]\n",
      " [-0.10941005]]\n",
      "3200 0.69359696 [[-0.05106806]\n",
      " [-0.10457182]]\n",
      "3300 0.69356114 [[-0.04969628]\n",
      " [-0.0999578 ]]\n",
      "3400 0.6935283 [[-0.04834132]\n",
      " [-0.09555701]]\n",
      "3500 0.693498 [[-0.04700467]\n",
      " [-0.09135909]]\n",
      "3600 0.69347024 [[-0.04568771]\n",
      " [-0.08735422]]\n",
      "3700 0.6934447 [[-0.04439157]\n",
      " [-0.08353305]]\n",
      "3800 0.69342124 [[-0.04311728]\n",
      " [-0.07988671]]\n",
      "3900 0.6933997 [[-0.04186569]\n",
      " [-0.07640681]]\n",
      "4000 0.69337994 [[-0.04063753]\n",
      " [-0.0730854 ]]\n",
      "4100 0.69336164 [[-0.03943339]\n",
      " [-0.0699148 ]]\n",
      "4200 0.69334483 [[-0.03825375]\n",
      " [-0.06688792]]\n",
      "4300 0.6933294 [[-0.03709902]\n",
      " [-0.06399789]]\n",
      "4400 0.69331515 [[-0.03596947]\n",
      " [-0.06123818]]\n",
      "4500 0.69330204 [[-0.0348653 ]\n",
      " [-0.05860265]]\n",
      "4600 0.69328994 [[-0.03378664]\n",
      " [-0.05608542]]\n",
      "4700 0.6932789 [[-0.03273353]\n",
      " [-0.05368089]]\n",
      "4800 0.69326866 [[-0.03170596]\n",
      " [-0.05138382]]\n",
      "4900 0.69325924 [[-0.03070386]\n",
      " [-0.04918916]]\n",
      "5000 0.6932505 [[-0.02972709]\n",
      " [-0.0470921 ]]\n",
      "5100 0.6932425 [[-0.0287755 ]\n",
      " [-0.04508811]]\n",
      "5200 0.69323516 [[-0.02784885]\n",
      " [-0.04317284]]\n",
      "5300 0.69322836 [[-0.0269469]\n",
      " [-0.0413422]]\n",
      "5400 0.69322205 [[-0.02606937]\n",
      " [-0.03959224]]\n",
      "5500 0.6932164 [[-0.02521594]\n",
      " [-0.0379193 ]]\n",
      "5600 0.69321096 [[-0.02438627]\n",
      " [-0.03631974]]\n",
      "5700 0.69320613 [[-0.02358   ]\n",
      " [-0.03479025]]\n",
      "5800 0.69320154 [[-0.02279674]\n",
      " [-0.03332759]]\n",
      "5900 0.6931974 [[-0.0220361 ]\n",
      " [-0.03192871]]\n",
      "6000 0.69319355 [[-0.02129766]\n",
      " [-0.03059073]]\n",
      "6100 0.69319004 [[-0.02058098]\n",
      " [-0.02931087]]\n",
      "6200 0.6931868 [[-0.01988566]\n",
      " [-0.02808645]]\n",
      "6300 0.6931837 [[-0.01921122]\n",
      " [-0.02691502]]\n",
      "6400 0.6931809 [[-0.01855723]\n",
      " [-0.02579414]]\n",
      "6500 0.69317836 [[-0.01792324]\n",
      " [-0.02472157]]\n",
      "6600 0.6931759 [[-0.0173088 ]\n",
      " [-0.02369511]]\n",
      "6700 0.6931737 [[-0.01671345]\n",
      " [-0.0227127 ]]\n",
      "6800 0.69317174 [[-0.01613669]\n",
      " [-0.02177237]]\n",
      "6900 0.69316983 [[-0.01557812]\n",
      " [-0.02087224]]\n",
      "7000 0.69316804 [[-0.01503726]\n",
      " [-0.02001052]]\n",
      "7100 0.69316655 [[-0.01451366]\n",
      " [-0.01918552]]\n",
      "7200 0.69316506 [[-0.01400688]\n",
      " [-0.01839559]]\n",
      "7300 0.6931637 [[-0.01351646]\n",
      " [-0.01763919]]\n",
      "7400 0.69316244 [[-0.01304196]\n",
      " [-0.01691483]]\n",
      "7500 0.69316125 [[-0.01258297]\n",
      " [-0.01622112]]\n",
      "7600 0.6931602 [[-0.01213903]\n",
      " [-0.0155567 ]]\n",
      "7700 0.69315916 [[-0.01170974]\n",
      " [-0.01492028]]\n",
      "7800 0.69315827 [[-0.01129467]\n",
      " [-0.01431064]]\n",
      "7900 0.69315743 [[-0.01089341]\n",
      " [-0.0137266 ]]\n",
      "8000 0.6931566 [[-0.01050558]\n",
      " [-0.01316707]]\n",
      "8100 0.6931559 [[-0.01013077]\n",
      " [-0.01263096]]\n",
      "8200 0.6931552 [[-0.00976861]\n",
      " [-0.01211727]]\n",
      "8300 0.6931546 [[-0.00941869]\n",
      " [-0.01162502]]\n",
      "8400 0.69315404 [[-0.00908066]\n",
      " [-0.01115329]]\n",
      "8500 0.69315356 [[-0.00875417]\n",
      " [-0.01070118]]\n",
      "8600 0.6931531 [[-0.00843885]\n",
      " [-0.01026786]]\n",
      "8700 0.69315267 [[-0.00813436]\n",
      " [-0.00985251]]\n",
      "8800 0.69315225 [[-0.00784036]\n",
      " [-0.00945438]]\n",
      "8900 0.69315183 [[-0.00755652]\n",
      " [-0.00907272]]\n",
      "9000 0.6931515 [[-0.00728252]\n",
      " [-0.00870683]]\n",
      "9100 0.69315124 [[-0.00701805]\n",
      " [-0.00835603]]\n",
      "9200 0.6931509 [[-0.0067628 ]\n",
      " [-0.00801969]]\n",
      "9300 0.6931505 [[-0.00651648]\n",
      " [-0.00769719]]\n",
      "9400 0.6931503 [[-0.00627878]\n",
      " [-0.00738794]]\n",
      "9500 0.69315004 [[-0.00604945]\n",
      " [-0.00709138]]\n",
      "9600 0.69314986 [[-0.00582819]\n",
      " [-0.00680698]]\n",
      "9700 0.6931497 [[-0.00561475]\n",
      " [-0.00653422]]\n",
      "9800 0.6931495 [[-0.00540887]\n",
      " [-0.00627261]]\n",
      "9900 0.6931493 [[-0.00521029]\n",
      " [-0.00602168]]\n",
      "10000 0.6931491 [[-0.00501877]\n",
      " [-0.00578099]]\n",
      "\n",
      "Hypothesis:  [[0.5016013 ]\n",
      " [0.500156  ]\n",
      " [0.50034654]\n",
      " [0.49890128]] \n",
      "Correct:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val, w_val = sess.run([train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val, w_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "\n",
    "# method 1\n",
    "# But it doesn't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9085834 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "100 0.69060224 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "200 0.6891924 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "300 0.68761295 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "400 0.68576175 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "500 0.6835504 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "600 0.68088055 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "700 0.67764425 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "800 0.67372936 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "900 0.6690323 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1000 0.6634764 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1100 0.65703136 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1200 0.6497305 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1300 0.6416788 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1400 0.63304734 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1500 0.624054 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1600 0.61493456 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1700 0.6059122 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1800 0.5971742 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "1900 0.58885884 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2000 0.5810536 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2100 0.5738003 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2200 0.56710315 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2300 0.56093794 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2400 0.55525696 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2500 0.5499926 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2600 0.5450541 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2700 0.5403206 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2800 0.53562677 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "2900 0.53074133 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3000 0.5253395 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3100 0.51897615 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3200 0.5110608 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3300 0.500818 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3400 0.48716038 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3500 0.4684434 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3600 0.44251263 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3700 0.40802333 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3800 0.36637866 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "3900 0.32153195 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4000 0.2779702 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4100 0.23892651 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4200 0.20577207 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4300 0.17845482 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4400 0.15622321 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4500 0.13814586 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4600 0.123361185 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4700 0.11115843 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4800 0.10098025 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "4900 0.09239891 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5000 0.08508825 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5100 0.07879911 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5200 0.07333962 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5300 0.06856124 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5400 0.064347446 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5500 0.060606107 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5600 0.05726361 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5700 0.054260507 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5800 0.051548444 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "5900 0.049087614 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6000 0.046845272 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6100 0.044793725 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6200 0.042910002 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6300 0.041174512 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6400 0.0395706 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6500 0.038083997 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6600 0.036702424 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6700 0.03541526 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6800 0.034213178 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "6900 0.03308825 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7000 0.032033145 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7100 0.031041712 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7200 0.030108424 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7300 0.02922835 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7400 0.028397117 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7500 0.027610755 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7600 0.02686587 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7700 0.026159152 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7800 0.025487898 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "7900 0.024849467 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8000 0.024241537 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8100 0.02366199 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8200 0.023108887 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8300 0.022580525 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8400 0.022075266 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8500 0.021591611 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8600 0.021128308 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8700 0.02068403 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8800 0.020257678 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "8900 0.019848138 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9000 0.01945457 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9100 0.019075906 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9200 0.018711448 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9300 0.018360388 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9400 0.018021982 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9500 0.017695589 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9600 0.01738055 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9700 0.017076332 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9800 0.01678236 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "9900 0.016498169 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "10000 0.016223226 [[ 1.4151576 ]\n",
      " [-0.47953194]]\n",
      "\n",
      "Hypothesis:  [[0.01572677]\n",
      " [0.98030055]\n",
      " [0.9845745 ]\n",
      " [0.01349661]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([2]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val, w_val = sess.run([train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val, w_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "    \n",
    "# method 2\n",
    "# good work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7252467 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "100 0.68090016 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "200 0.6682054 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "300 0.65359676 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "400 0.63572943 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "500 0.6135013 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "600 0.5860992 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "700 0.5529938 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "800 0.5140435 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "900 0.46978503 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1000 0.42169696 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1100 0.3721639 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1200 0.32401162 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1300 0.279723 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1400 0.24080256 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1500 0.2076803 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1600 0.18002595 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1700 0.15714014 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1800 0.13823247 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "1900 0.12256604 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2000 0.1095127 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2100 0.098560035 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2200 0.089299366 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2300 0.08140789 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2400 0.07463132 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2500 0.068769306 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2600 0.06366275 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2700 0.05918517 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2800 0.05523487 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "2900 0.051729828 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3000 0.04860334 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3100 0.04580064 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3200 0.043276604 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3300 0.040993858 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3400 0.038921162 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3500 0.03703201 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3600 0.035304274 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3700 0.033718977 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3800 0.03226 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "3900 0.030913418 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4000 0.02966724 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4100 0.028511126 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4200 0.027435957 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4300 0.026433859 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4400 0.025497856 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4500 0.024621913 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4600 0.02380064 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4700 0.023029182 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4800 0.022303289 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "4900 0.021619234 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5000 0.020973507 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5100 0.020363167 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5200 0.019785471 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5300 0.019237872 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5400 0.018718239 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5500 0.01822447 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5600 0.017754726 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5700 0.017307423 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5800 0.016880985 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "5900 0.01647405 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6000 0.01608527 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6100 0.015713573 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6200 0.0153578855 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6300 0.015017135 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6400 0.014690472 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6500 0.014377128 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6600 0.014076257 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6700 0.013787106 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6800 0.013509105 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "6900 0.013241611 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7000 0.012984027 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7100 0.012735815 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7200 0.012496591 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7300 0.012265714 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7400 0.012042906 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7500 0.011827741 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7600 0.011619759 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7700 0.011418654 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7800 0.011224181 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "7900 0.01103587 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8000 0.010853534 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8100 0.010676931 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8200 0.010505738 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8300 0.0103397295 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8400 0.010178676 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8500 0.0100223785 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8600 0.009870624 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8700 0.009723215 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8800 0.009579984 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "8900 0.009440748 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9000 0.009305324 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9100 0.009173638 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9200 0.009045445 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9300 0.008920684 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9400 0.008799145 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9500 0.008680793 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9600 0.008565494 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9700 0.008453066 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9800 0.008343449 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "9900 0.008236534 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "10000 0.008132232 [[-1.3512232 ]\n",
      " [ 0.15260983]]\n",
      "\n",
      "Hypothesis:  [[0.00491855]\n",
      " [0.9902934 ]\n",
      " [0.99355525]\n",
      " [0.01131021]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val, w_val = sess.run([train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val, w_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "    \n",
    "# method 3\n",
    "# good work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8969641 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "100 0.67829585 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "200 0.66455877 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "300 0.6481752 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "400 0.62758064 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "500 0.6012431 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "600 0.5678216 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "700 0.5267123 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "800 0.47871023 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "900 0.42626688 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1000 0.3729329 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1100 0.3222191 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1200 0.2766099 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1300 0.23723844 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1400 0.20414202 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1500 0.1767219 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1600 0.15413292 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1700 0.13551761 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1800 0.1201137 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "1900 0.10728667 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2000 0.09652602 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2100 0.08742729 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2200 0.07967277 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2300 0.073012635 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2400 0.06725015 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2500 0.0622295 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2600 0.057826508 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2700 0.05394158 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2800 0.050494254 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "2900 0.04741899 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3000 0.04466217 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3100 0.042179495 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3200 0.039934177 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3300 0.03789551 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3400 0.03603752 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3500 0.034338452 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3600 0.032779545 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3700 0.03134503 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3800 0.030021202 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "3900 0.028796274 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4000 0.027660012 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4100 0.0266035 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4200 0.025618955 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4300 0.024699535 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4400 0.023839258 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4500 0.02303276 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4600 0.022275396 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4700 0.021562925 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4800 0.020891568 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "4900 0.020258036 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5000 0.019659352 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5100 0.019092763 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5200 0.018555861 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5300 0.018046474 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5400 0.017562514 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5500 0.017102286 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5600 0.016664112 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5700 0.016246429 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5800 0.015847985 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "5900 0.015467425 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6000 0.015103624 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6100 0.014755566 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6200 0.014422281 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6300 0.0141028175 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6400 0.013796406 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6500 0.013502281 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6600 0.013219689 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6700 0.012948094 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6800 0.01268676 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "6900 0.012435244 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7000 0.01219292 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7100 0.011959341 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7200 0.011734111 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7300 0.01151668 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7400 0.011306774 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7500 0.011103965 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7600 0.010907932 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7700 0.010718295 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7800 0.010534853 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "7900 0.010357212 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8000 0.01018516 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8100 0.010018481 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8200 0.009856856 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8300 0.009700073 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8400 0.00954798 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8500 0.009400317 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8600 0.009256948 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8700 0.009117631 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8800 0.008982288 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "8900 0.008850692 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9000 0.008722691 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9100 0.008598149 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9200 0.008476929 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9300 0.008358925 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9400 0.008244031 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9500 0.008132094 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9600 0.008022994 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9700 0.007916625 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9800 0.007812941 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "9900 0.007711776 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "10000 0.007613099 [[-1.6678444]\n",
      " [ 1.2432384]]\n",
      "\n",
      "Hypothesis:  [[0.00827268]\n",
      " [0.9928275 ]\n",
      " [0.99159074]\n",
      " [0.00647722]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W4) + b4)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val, w_val = sess.run([train, cost, W], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val, w_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
    "    \n",
    "# method 4\n",
    "# good work!\n",
    "# Performance ranking     method 1 < method 2 < method 3 < method 4 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
